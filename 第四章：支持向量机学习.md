## 第四章：支持向量机学习 + 神经网络学习

### MOOC笔记

--------

#### 1. 最大边缘超平面

 	· 线性分类器基本想法是：在样本空间中寻找一个超平面将不同的样本数据分开，图中B1就是划分矩形样本点和圆形样本点的一个超平面

<img src="C:\Users\13411\AppData\Roaming\Typora\typora-user-images\image-20200318212720699.png" alt="image-20200318212720699" style="zoom:40%;" />

· 但是这样的超平面可能不止一个，分类器需要做的是选取最优的超平面来表示决策边界。衡量的标准就是：**超平面的边缘**。**这是因为具有较大边缘的决策边界比那些具有较小边缘的决策边界具有更好的泛化误差，对于扰动的抵抗性更强。**例如下图中超平面B1的边缘明显大于B2的边缘

<img src="C:\Users\13411\AppData\Roaming\Typora\typora-user-images\image-20200318212943547.png" alt="image-20200318212943547" style="zoom:40%;" />

· 根据结构风险最小化原理，需要设计最大化决策边界的边缘的线性分类器以确保最坏情况下的泛化误差最小，线性支持向量机就是这样一个分类器。

#### 2.线性支持向量机

· 给定训练数据集，线性分类器决策边界的线性方程可以写为：

​										$$\omega^T*x + b = 0 $$

​	$\omega$ 是法向量决定了决策边界的方向，b 是位移量决定了决策边界与圆点之间的距离

正确的分类器满足：

<img src="C:\Users\13411\AppData\Roaming\Typora\typora-user-images\image-20200318220729188.png" alt="image-20200318220729188" style="zoom:40%;" />

调整$\omega$ 和 b的值使得满足：（$y_i$ 代表目标函数预测值，+1 为正例，-1为反例）

​	<img src="C:\Users\13411\AppData\Roaming\Typora\typora-user-images\image-20200318221512602.png" alt="image-20200318221512602" style="zoom:40%;" />

两个异类支持向量到决策边界的距离之和称为决策边界的边缘。支持向量机学习就是寻找合适的w和b使得决策边界的边缘最大化。

具体的约束优化函数如下：

<img src="C:\Users\13411\AppData\Roaming\Typora\typora-user-images\image-20200318224256955.png" alt="image-20200318224256955" style="zoom:40%;" />

以此将线性支持向量机的学习问题转化为一个凸二次优化问题，可以利用现成的**优化计算包**解决。也可以利用**拉格朗日乘子法**解决

#### 拉格朗日乘子法

<img src="C:\Users\13411\AppData\Roaming\Typora\typora-user-images\image-20200318230335890.png" alt="image-20200318230335890" style="zoom:33%;" />

将对偶优化问题转变为如下的最小优化问题

<img src="C:\Users\13411\AppData\Roaming\Typora\typora-user-images\image-20200318232407239.png" alt="image-20200318232407239" style="zoom:33%;" />

随后将不等式约束转化为等式约束：

<img src="C:\Users\13411\AppData\Roaming\Typora\typora-user-images\image-20200318234103417.png" alt="image-20200318234103417" style="zoom:33%;" />

其中KKT详解可参照：

https://zhuanlan.zhihu.com/p/38163970

已上可得出结论如下图：==最终支持向量机模型的参数w和b只依赖于支持向量==

<img src="C:\Users\13411\AppData\Roaming\Typora\typora-user-images\image-20200318234546191.png" alt="image-20200318234546191" style="zoom:33%;" />



#### 非线性支持向量机

​	在实际运用中，很少存在理想的线性决策边界将所有训练样本正确分类，因此需要将样本空间映射到更高维的特征空间，使得样本在高维空间内线性可分。

==如果原始空间是有限维，那么一定存在一个更高维的特征空间使得样本线性可分==

​	将线性支持向量机作为相应的映射变换可得到非线性支持向量机。映射后向量为  $\phi$(x) 且有

<img src="C:\Users\13411\AppData\Roaming\Typora\typora-user-images\image-20200319232713669.png" alt="image-20200319232713669" style="zoom:33%;" />

其中设计计算 $\phi(xi)^T\phi(x_j)$ 的内积计算会很复杂。为解决该问题，通常是不显式地设计  $\phi$（·）而是设计一个核函数：

- $$K(x_i,x_j) = \phi(x_i)^T\phi(x_j)$$

  

核函数的选取条件是：==只要一个对称函数所对应的核矩阵半正定那么就能作为核函数使用。==（MERCER定理）

·常用核函数有：

<img src="C:\Users\13411\AppData\Roaming\Typora\typora-user-images\image-20200319233436440.png" alt="image-20200319233436440" style="zoom:33%;" />



### 教材内容补充学习

-------------------------

#### 1.间隔与支持向量

#### 2.对偶问题

#### 3.核函数

​	有了核函数之后可以将$\omega和b$表示为： 

<img src="C:\Users\13411\AppData\Roaming\Typora\typora-user-images\image-20200320000958924.png" alt="image-20200320000958924" style="zoom:50%;" />

这一展开式也称为：支持向量展式

·核函数的组合也可以为核函数：

<img src="C:\Users\13411\AppData\Roaming\Typora\typora-user-images\image-20200320001334628.png" alt="image-20200320001334628" style="zoom:50%;" />

#### 4.软间隔与正则化

现实任务中往往很难确定合适的核函数使得训练样本在特征空间中线性课分；退一步说即使恰好找到了某个确定的核函数也很难断定这个貌似线性可分的结果是不是由于==过拟合==造成的。因此引入==软间隔==的概念soft margin

<img src="C:\Users\13411\AppData\Roaming\Typora\typora-user-images\image-20200320002818053.png" alt="image-20200320002818053" style="zoom:53%;" />



软间隔允许某些样本不满足约束：<img src="C:\Users\13411\AppData\Roaming\Typora\typora-user-images\image-20200320002841554.png" alt="image-20200320002841554" style="zoom:45%;" />

但是这些不满足的样本应该尽可能少，于是优化目标可写为：

<img src="C:\Users\13411\AppData\Roaming\Typora\typora-user-images\image-20200320002922473.png" alt="image-20200320002922473" style="zoom:50%;" />

其中 C>0 是一个常数，$\iota_{(0/1)}$ 为"0/1损失函数"

<img src="C:\Users\13411\AppData\Roaming\Typora\typora-user-images\image-20200320003141399.png" alt="image-20200320003141399" style="zoom:55%;" />

但是  $\iota$  函数的性质不够好，人们通常使用别的函数来代替它，称为“替代损失”surrogate loss。替代损失函数一般具有较好的数学性质，例如通常为连续的凸函数：下图为常用替代损失函数

<img src="C:\Users\13411\AppData\Roaming\Typora\typora-user-images\image-20200320003424633.png" alt="image-20200320003424633" style="zoom:50%;" />

<img src="C:\Users\13411\AppData\Roaming\Typora\typora-user-images\image-20200320003609280.png" alt="image-20200320003609280" style="zoom:50%;" />

引入松弛变量（slack variables），通过拉格朗日乘子法可得：

<img src="C:\Users\13411\AppData\Roaming\Typora\typora-user-images\image-20200320003723371.png" alt="image-20200320003723371" style="zoom:50%;" />

其中 $\alpha_i 和 \mu_i $ 均≥ 0 为拉格朗日乘子。

<img src="C:\Users\13411\AppData\Roaming\Typora\typora-user-images\image-20200320003917799.png" alt="image-20200320003917799" style="zoom:50%;" />

<img src="C:\Users\13411\AppData\Roaming\Typora\typora-user-images\image-20200320003955872.png" alt="image-20200320003955872" style="zoom:50%;" />

我们还可以把式(6.29) 中的 0/1 损失函数换成别的替代损失函数以得到 其他学习模型，这些模型的性质与所用的替代函数直接相关，但它们具有一 个共性:优化目标中的第一项用来描述划分超平面的"间隔"大小，另一项 <img src="C:\Users\13411\AppData\Roaming\Typora\typora-user-images\image-20200320004215492.png" alt="image-20200320004215492" style="zoom:33%;" /> 用来表述训练集上的误差，可写为更一般的形式

<img src="C:\Users\13411\AppData\Roaming\Typora\typora-user-images\image-20200320004225547.png" alt="image-20200320004225547" style="zoom: 33%;" />![image-20200320004556055](C:\Users\13411\AppData\Roaming\Typora\typora-user-images\image-20200320004556055.png)

$\Omega(f)$ 称为==结构风险==（structural risk）,用来描述模型 f 的某些性质；第二项![image-20200320004556055](C:\Users\13411\AppData\Roaming\Typora\typora-user-images\image-20200320004556055.png)称为==经验风险== empirical  risk,用来描述模型与训练数据的契合程度；C 用于对两者进行折中

#### 5、支持向量回归

支持向量回归（Support Vector Regression)简称SVR。传统回归模型通常基于模型输出f(x) 和 真实输出Y 之间的差别来计算损失，当且仅当f(x) 与y 完全相同时损失才为零。SVR则与此不同，假设我们能接受f(X) 与 Y之间有 $\varepsilon$ 的偏差， 则当 $f(x) 与 y 之间的差大于\varepsilon$ 时才计算损失。如图，这相当于以f(x) 为中心，构建一个宽度为$2 \varepsilon$ 的 间隔带，若训练样本落入该间隔带直接正确判定处理。

于是SVR问题可形化为：

<img src="C:\Users\13411\AppData\Roaming\Typora\typora-user-images\image-20200320010255400.png" alt="image-20200320010255400" style="zoom:50%;" />

C为正则化常数,$\iota_\varepsilon$ 是不敏感损失函数（$\varepsilon$ - insensitive loss)

<img src="C:\Users\13411\AppData\Roaming\Typora\typora-user-images\image-20200320010432724.png" alt="image-20200320010432724" style="zoom:50%;" />

<img src="C:\Users\13411\AppData\Roaming\Typora\typora-user-images\image-20200320010531389.png" alt="image-20200320010531389" style="zoom:47%;" />

最终得：SVR的解形如<img src="C:\Users\13411\AppData\Roaming\Typora\typora-user-images\image-20200320011642152.png" alt="image-20200320011642152" style="zoom:50%;" />

能使得<img src="C:\Users\13411\AppData\Roaming\Typora\typora-user-images\image-20200320011702316.png" alt="image-20200320011702316" style="zoom:33%;" />成立的样本即为SVR的支持向量，它们一定落在间隔带之外。

SVR可进一步表示为：

<img src="C:\Users\13411\AppData\Roaming\Typora\typora-user-images\image-20200320011854610.png" alt="image-20200320011854610" style="zoom:50%;" />

#### 6、核方法

SVM与SVR学得的模型总能表示成核函数的线性结合。引入表示定理（representer theorem）：

><img src="C:\Users\13411\AppData\Roaming\Typora\typora-user-images\image-20200320012412942.png" alt="image-20200320012412942" style="zoom:50%;" />

表示定理中对正则化项仅要求单调递增，==意味着对于一般的损失函数核正则化项，优化问题的最优解都可以表示为核函数的线性组合。==

​	核化：引入核函数，是一种常见的核方法，可以将线性学习器拓展为非线性学习器。从而得到”核线性判别分析“KLDA



### 神经网络

--------

#### 神经网络的定义

​	定义多种多样，最广泛接受的是：

	>由具有适应性的简单单元组成的广泛并行互联的网络，它的组织能够模拟生物神经系统对真实世界物体做出的反应。

#### 神经网络发展史

> 1）萌芽期（人类研究自己智能的开始~1949）
>
>  1943年, McCulloch和Pitts 提出第一个神经元数学模型, 即M-P 模型, 并从原理上证明了人工神经网络 能够计算任何算数和逻辑函数。 1949年, Hebb 发表《The Organization of Behavior》一书, 提出生物神经元学习的机理, 即Hebb学 习规则。Hebb学习规律被认为是神经网络学习算法的里程碑。 
>
> 2）第一次高潮期（1950~1968） 
>
> 1958年, Rosenblatt提出了单层感知机(Perceptron)模型及其学习规则。单层感知机的成功标示着神 经网络研究的第一高潮期的到来。
>
> 3）反思期（1969~1982） 
>
> 1969年，Minsky和 Papert发表了《Perceptrons》一书, 明确指出：单层感知机不能解决非线性问 题, 多层网络的训练算法尚无希望。 很多献身于神经网络研究的科学家的研究结果很难得到发表，不少有意义的研究成果即使发表了，也 很难被同行看到，著名的BP算法的研究就是一个典型的例子。 
>
> 4）第二次高潮期（1983~1990） 
>
> 1982年，Hopfield提出了循环网络：引入李雅普诺夫(Lyapunov)函数作为网络性能判定的能量函 数，建立了神经网络稳定性的判别依据。 1984年，Hopfield设计并实现了后来被人们称为 Hopfield网络的电路。较好地解决了著名的TSP问 题，找到了最佳解的近似解。 1986年，PDP小组的Rumelhart等研究者重新独立地提出了多层神经网络的学习算法—BP算法，较 好地解决了多层神经网络的学习问题。 
>
> 5）再认识与应用期（1991~） 2006年，Hinton提出了深度信念网络(DBN), 通过“ 预训练+微调”使得深度模型的最优化变得相对容 易。 2012年，Hinton组参加ImageNet竞赛，使用CNN模型以超过第二名10个百分点的成绩夺得当年竞 赛的冠军。 伴随云计算、大数据时代的到来，计算能力的大幅提升，使得深度学习模型在计算机视觉、自然语言 处理、语音识别等众多领域都取得了较大的成功。